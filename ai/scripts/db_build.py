"""
ë²•ë ¹ ë°ì´í„° ë²¡í„° DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ v2 - ê³„ì¸µì  êµ¬ì¡° ì§€ì›

.dataset/ko-law-parser/law/*.json íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬
law_sources_v2 í…Œì´ë¸”ì— ê³„ì¸µ ì •ë³´ì™€ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. ê³„ì¸µ êµ¬ì¡° ì¶”ì  (parent_id, article_id, level)
2. ìƒìœ„ ê³„ì¸µ ì •ë³´ë¥¼ í¬í•¨í•œ full_text_for_embedding ìƒì„±
3. ì§§ì€ í…ìŠ¤íŠ¸ë„ ì˜ë¯¸ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ í¬í•¨

Usage:
    cd /path/to/gift-tax-agent
    source .venv/bin/activate
    python -m ai.scripts.build_law_vector_db_v2
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import re
import sys
from pathlib import Path
from typing import Iterator

# Backend ì„í¬íŠ¸
_PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(_PROJECT_ROOT / "backend"))

from chalicelib.db.connection import get_db_session
from chalicelib.models.database import LawSourceV2

# AI ì„í¬íŠ¸
from ai.clients.gemini import GeminiClient
from ai.config import GeminiSettings

logging.basicConfig(
    level=logging.INFO,
    format="[%(levelname)s] %(message)s",
)
LOGGER = logging.getLogger(__name__)


class LawNode:
    """ë²•ë ¹ ê³„ì¸µ ë…¸ë“œ"""

    def __init__(
        self,
        law_name: str,
        key: str,
        content: str,
        parent: LawNode | None = None,
        level: str = "article",
    ):
        self.law_name = law_name
        self.key = key  # "ì œ53ì¡°(ì¦ì—¬ì¬ì‚° ê³µì œ)", "1í•­ ë°°ìš°ìë¡œë¶€í„°..."
        self.content = content
        self.parent = parent
        self.level = level
        self.children: list[LawNode] = []

        # article_id ì¶”ì¶œ
        self.article_id = self._extract_article_id()

    def _extract_article_id(self) -> str | None:
        """ì¡° ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: "ì œ53ì¡°" -> "53", "ì œ53ì¡°ì˜2" -> "53-2")"""
        match = re.search(r"ì œ(\d+)ì¡°(?:ì˜(\d+))?", self.key)
        if match:
            article_num = match.group(1)
            sub_num = match.group(2)
            return f"{article_num}-{sub_num}" if sub_num else article_num

        # ë¶€ëª¨ê°€ ìˆìœ¼ë©´ ë¶€ëª¨ì˜ article_id ìƒì†
        if self.parent:
            return self.parent.article_id
        return None

    def get_hierarchy_path(self) -> str:
        """ê³„ì¸µ ê²½ë¡œ ìƒì„± (ì˜ˆ: "ì œ53ì¡°(ì¦ì—¬ì¬ì‚° ê³µì œ) > 2í•­")"""
        path_parts = []
        node = self
        while node:
            # êµ¬ì¡° í‚¤ì›Œë“œë§Œ í¬í•¨
            if self._is_structural_keyword(node.key):
                path_parts.insert(0, node.key)
            node = node.parent
        return " > ".join(path_parts)

    def get_full_text_for_embedding(self) -> str:
        """ì„ë² ë”©ìš© ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„± (ìƒìœ„ ê³„ì¸µ ì •ë³´ í¬í•¨)"""
        # ê³„ì¸µ ê²½ë¡œ + í˜„ì¬ content
        path = self.get_hierarchy_path()
        if path:
            return f"{path}\n{self.content}"
        return self.content

    def get_full_reference(self) -> str:
        """ì „ì²´ ì°¸ì¡° ê²½ë¡œ (ë²•ë ¹ëª… í¬í•¨)"""
        path_parts = [self.law_name]
        node = self
        while node:
            if self._is_structural_keyword(node.key):
                path_parts.append(node.key)
            node = node.parent
        return " ".join(reversed(path_parts))

    @staticmethod
    def _is_structural_keyword(text: str) -> bool:
        """êµ¬ì¡°ì  í‚¤ì›Œë“œì¸ì§€ í™•ì¸"""
        keywords = ["ì œ", "ì¥", "í¸", "ì ˆ", "ê´€", "í•­", "í˜¸", "ëª©"]
        return any(kw in text for kw in keywords) and len(text) < 100


def parse_law_json_hierarchical(
    law_name: str,
    data: dict,
    source_file: str,
    parent: LawNode | None = None,
) -> Iterator[LawNode]:
    """
    ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©° ì¬ê·€ì  íŒŒì‹±

    Args:
        law_name: ë²•ë ¹ëª…
        data: JSON ë”•ì…”ë„ˆë¦¬
        source_file: ì›ë³¸ íŒŒì¼ ê²½ë¡œ
        parent: ë¶€ëª¨ ë…¸ë“œ

    Yields:
        LawNode ê°ì²´
    """

    for key, value in data.items():
        # ë ˆë²¨ íŒë‹¨
        level = determine_level(key)

        # í˜„ì¬ ë…¸ë“œ ìƒì„±
        node = LawNode(
            law_name=law_name,
            key=key,
            content=key,  # ì¼ë‹¨ keyë¥¼ contentë¡œ
            parent=parent,
            level=level,
        )

        if parent:
            parent.children.append(node)

        if isinstance(value, dict):
            if value:
                # í•˜ìœ„ í•­ëª©ì´ ìˆìŒ - ì¬ê·€ íƒìƒ‰
                yield node
                yield from parse_law_json_hierarchical(law_name, value, source_file, node)
            else:
                # ë¦¬í”„ ë…¸ë“œ (ë³¸ë¬¸ í…ìŠ¤íŠ¸)
                # keyê°€ ê¸´ í…ìŠ¤íŠ¸ë¼ë©´ ì´ê²ƒì´ ì‹¤ì œ ë‚´ìš©
                if len(key) >= 10:
                    node.content = key
                    yield node


def determine_level(key: str) -> str:
    """í‚¤ì›Œë“œë¡œ ê³„ì¸µ ë ˆë²¨ íŒë‹¨"""
    if re.search(r"ì œ\d+ì¡°", key):
        return "article"
    elif re.search(r"\d+í•­", key):
        return "paragraph"
    elif re.search(r"\d+í˜¸", key):
        return "item"
    elif re.search(r"[ê°€-í£]ëª©", key):
        return "subitem"
    else:
        # ì¥, ì ˆ ë“±ì€ articleë¡œ ë¶„ë¥˜
        if any(kw in key for kw in ["ì¥", "ì ˆ", "í¸", "ê´€"]):
            return "article"
        return "article"  # ê¸°ë³¸ê°’


def compute_chunk_hash(law_name: str, full_ref: str, content: str) -> str:
    """ì²­í¬ í•´ì‹œ ìƒì„±"""
    data = f"{law_name}|{full_ref}|{content}"
    return hashlib.sha256(data.encode("utf-8")).hexdigest()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    LOGGER.info("=" * 60)
    LOGGER.info("ë²•ë ¹ ë²¡í„° DB êµ¬ì¶• v2 ì‹œì‘")
    LOGGER.info("=" * 60)

    # ì„¤ì • ë¡œë“œ
    try:
        settings = GeminiSettings.from_env()
    except ValueError as e:
        LOGGER.error(f"í™˜ê²½ ë³€ìˆ˜ ì˜¤ë¥˜: {e}")
        sys.exit(1)

    client = GeminiClient(settings)

    # ë²•ë ¹ íŒŒì¼ ê²€ìƒ‰
    law_dir = _PROJECT_ROOT / ".dataset" / "ko-law-parser" / "law"
    if not law_dir.exists():
        LOGGER.error(f"ë²•ë ¹ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {law_dir}")
        sys.exit(1)

    # ìƒì†ì„¸ë°ì¦ì—¬ì„¸ë²• ê´€ë ¨ë§Œ ì²˜ë¦¬
    target_laws = [
        "ìƒì†ì„¸ë°ì¦ì—¬ì„¸ë²•.json",
        "ìƒì†ì„¸ë°ì¦ì—¬ì„¸ë²•ì‹œí–‰ë ¹.json",
        "ìƒì†ì„¸ë°ì¦ì—¬ì„¸ë²•ì‹œí–‰ê·œì¹™.json",
    ]

    json_files = [law_dir / name for name in target_laws if (law_dir / name).exists()]
    LOGGER.info(f"ì²˜ë¦¬ ëŒ€ìƒ ë²•ë ¹: {len(json_files)}ê°œ")

    total_inserted = 0
    total_skipped = 0

    # íŒŒì¼ë³„ ì²˜ë¦¬
    for json_file in json_files:
        law_name = json_file.stem
        LOGGER.info("")
        LOGGER.info(f"ğŸ“– ì²˜ë¦¬ ì¤‘: {law_name}")

        # JSON ë¡œë“œ
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            LOGGER.error(f"  âŒ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

        # ê³„ì¸µ êµ¬ì¡° íŒŒì‹±
        nodes = list(parse_law_json_hierarchical(law_name, data, str(json_file)))
        LOGGER.info(f"  âœ“ {len(nodes)}ê°œ ë…¸ë“œ íŒŒì‹± ì™„ë£Œ")

        if not nodes:
            LOGGER.warning(f"  âš ï¸  ë…¸ë“œê°€ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        # ë°°ì¹˜ë³„ ì„ë² ë”© ë° INSERT
        batch_size = settings.embedding_batch_size

        # ë¨¼ì € parent_id ì—†ì´ ì €ì¥í•  ë…¸ë“œë“¤ (1ì°¨ INSERT)
        # ê·¸ ë‹¤ìŒ parent_id ì—…ë°ì´íŠ¸
        node_to_db_id = {}  # LawNode -> DB ID ë§¤í•‘

        with get_db_session() as db:
            for i in range(0, len(nodes), batch_size):
                batch = nodes[i : i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(nodes) + batch_size - 1) // batch_size

                LOGGER.info(f"  ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches}: {len(batch)}ê°œ ë…¸ë“œ ì²˜ë¦¬ ì¤‘...")

                # ì„ë² ë”© ìƒì„± (full_text_for_embedding ì‚¬ìš©)
                try:
                    texts = [node.get_full_text_for_embedding() for node in batch]
                    embeddings = await client.generate_embeddings_batch(texts)
                    LOGGER.info(f"     âœ“ ì„ë² ë”© ìƒì„± ì™„ë£Œ ({len(embeddings)}ê°œ)")
                except Exception as e:
                    LOGGER.error(f"     âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                    continue

                # DB INSERT
                batch_inserted = 0
                batch_skipped = 0

                for node, embedding in zip(batch, embeddings):
                    full_ref = node.get_full_reference()
                    chunk_hash = compute_chunk_hash(law_name, full_ref, node.content)

                    # ì¤‘ë³µ ì²´í¬
                    exists = (
                        db.query(LawSourceV2)
                        .filter(LawSourceV2.chunk_hash == chunk_hash)
                        .first()
                    )

                    if exists:
                        batch_skipped += 1
                        node_to_db_id[id(node)] = exists.id
                        continue

                    # parent_idëŠ” ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
                    try:
                        law_source = LawSourceV2(
                            chunk_hash=chunk_hash,
                            law_name=law_name,
                            full_reference=full_ref,
                            content=node.content,
                            parent_id=None,  # 1ì°¨ì—ëŠ” None
                            article_id=node.article_id,
                            level=node.level,
                            hierarchy_path=node.get_hierarchy_path(),
                            full_text_for_embedding=node.get_full_text_for_embedding(),
                            embedding=embedding,
                            source_file=str(json_file),
                            source_url=None,
                        )
                        db.add(law_source)
                        db.flush()  # ID ìƒì„±
                        node_to_db_id[id(node)] = law_source.id
                        batch_inserted += 1
                    except Exception as e:
                        LOGGER.error(f"     âŒ INSERT ì‹¤íŒ¨: {e}")
                        continue

                # ì»¤ë°‹
                try:
                    db.commit()
                    total_inserted += batch_inserted
                    total_skipped += batch_skipped
                    LOGGER.info(f"     âœ“ DB ì €ì¥ ì™„ë£Œ (ì¶”ê°€: {batch_inserted}, ìŠ¤í‚µ: {batch_skipped})")
                except Exception as e:
                    LOGGER.error(f"     âŒ DB ì»¤ë°‹ ì‹¤íŒ¨: {e}")
                    db.rollback()

            # 2ë‹¨ê³„: parent_id ì—…ë°ì´íŠ¸
            LOGGER.info("  ğŸ”„ parent_id ì—…ë°ì´íŠ¸ ì¤‘...")
            update_count = 0

            for node in nodes:
                if node.parent and id(node) in node_to_db_id and id(node.parent) in node_to_db_id:
                    child_id = node_to_db_id[id(node)]
                    parent_id = node_to_db_id[id(node.parent)]

                    db.query(LawSourceV2).filter(LawSourceV2.id == child_id).update(
                        {"parent_id": parent_id}
                    )
                    update_count += 1

            try:
                db.commit()
                LOGGER.info(f"     âœ“ parent_id {update_count}ê°œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            except Exception as e:
                LOGGER.error(f"     âŒ parent_id ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                db.rollback()

        LOGGER.info(f"  âœ… {law_name} ì²˜ë¦¬ ì™„ë£Œ")

    # ìµœì¢… ê²°ê³¼
    LOGGER.info("")
    LOGGER.info("=" * 60)
    LOGGER.info("âœ… ë²•ë ¹ ë²¡í„° DB v2 êµ¬ì¶• ì™„ë£Œ")
    LOGGER.info(f"   ì´ ì¶”ê°€: {total_inserted}ê°œ")
    LOGGER.info(f"   ì´ ìŠ¤í‚µ: {total_skipped}ê°œ")
    LOGGER.info("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
